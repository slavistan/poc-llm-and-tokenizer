{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./wizard-of-oz.txt\", encoding=\"utf-8-sig\") as f:\n",
    "    txt = f.read()\n",
    "\n",
    "VOCAB = sorted(list(set(txt)))\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "encode_dict = {c:i for i, c in enumerate(VOCAB)}\n",
    "decode_dict = {i:c for c, i in encode_dict.items()}\n",
    "\n",
    "def tok_encode(text):\n",
    "    return [encode_dict[c] for c in text]\n",
    "def tok_decode(tok_indices):\n",
    "    return \"\".join(decode_dict[e] for e in tok_indices)\n",
    "\n",
    "tok_decode(tok_encode(txt)) == txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "data = torch.LongTensor(tok_encode(txt)).to(DEVICE)\n",
    "split = int(len(data) * 0.9)\n",
    "train_data = data[:split]\n",
    "val_data = data[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Language Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from src import bigram\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = bigram.BigramLanguageModel(vocab_sz=VOCAB_SIZE).to(DEVICE)\n",
    "\n",
    "num_epochs = 16\n",
    "batch_size = 64\n",
    "dataset = bigram.BigramDataset(txt_tensor=train_data, device=DEVICE)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_samples = []\n",
    "samples_counter = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for num_epoch in range(num_epochs):\n",
    "    for ii, (x, y) in enumerate(dataloader):\n",
    "        loss = model.train_batch(x, y, optimizer)\n",
    "        samples_counter += len(x)\n",
    "\n",
    "        if (ii+1) % 1000 == 0 or ii + 1 == len(dataloader):\n",
    "            with torch.no_grad():\n",
    "                train_loss = model.compute_loss(train_data[:-1], train_data[1:])\n",
    "                val_loss = model.compute_loss(val_data[:-1], val_data[1:])\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                num_samples.append(samples_counter)\n",
    "\n",
    "            print(\"\\r\" * 100 + f\"epoch {num_epoch}: {ii+1}/{len(dataloader)}\", end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict({\n",
    "    \"num_samples\": num_samples,\n",
    "    \"train_losses\": train_losses,\n",
    "    \"val_losses\": val_losses,\n",
    "}).pivot_longer(column_names=[\"train_losses\", \"val_losses\"], names_to=\"what\")\n",
    "\n",
    "ggplot(df) + geom_line(aes(x=\"num_samples\", y=\"value\", color=\"what\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compute_loss(train_data[:-1], train_data[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lut = model.lut.weight.detach().to(\"cpu\").softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lektion\n",
    "\n",
    "- str.count() zählt keine überlappenden Muster!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram lut manuell\n",
    "\n",
    "lut_counting = torch.zeros((VOCAB_SIZE, VOCAB_SIZE))\n",
    "for c1, c2 in zip(train_data, train_data[1:]):\n",
    "    lut_counting[c1, c2] += 1\n",
    "# lut_counting = lut_counting.softmax(dim=-1)\n",
    "lut_counting = lut_counting / lut_counting.norm(dim=1, keepdim=True, p=1)\n",
    "lut_counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lut[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lut_counting[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "idx = np.random.randint(low=0, high=VOCAB_SIZE, size=(100,))\n",
    "out = \"\"\n",
    "for ii in range(len(idx)):\n",
    "    out += tok_decode([idx[ii]])\n",
    "\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_indices = model.generate(torch.tensor([0], device=DEVICE), 64)\n",
    "tok_decode(tok_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compute_loss(val_data[:-1], val_data[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-np.log(1/VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Language Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import neural\n",
    "model = neural.NeuralLanguageModel(vocab_sz=VOCAB_SIZE).to(DEVICE)\n",
    "\n",
    "num_epochs = 16\n",
    "batch_size = 64\n",
    "dataset = neural.NeuralDataset(txt_tensor=train_data, device=DEVICE)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_samples = []\n",
    "samples_counter = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for num_epoch in range(num_epochs):\n",
    "    for ii, (x, y) in enumerate(dataloader):\n",
    "        loss = model.train_batch(x, y, optimizer)\n",
    "        samples_counter += len(x)\n",
    "\n",
    "        if (ii+1) % 1000 == 0 or ii + 1 == len(dataloader):\n",
    "            with torch.no_grad():\n",
    "                train_loss = model.compute_loss(train_data[:-1], train_data[1:])\n",
    "                val_loss = model.compute_loss(val_data[:-1], val_data[1:])\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                num_samples.append(samples_counter)\n",
    "\n",
    "            print(\"\\r\" * 100 + f\"epoch {num_epoch}: {ii+1}/{len(dataloader)}\", end=\"\", flush=True)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
