{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./wizard-of-oz.txt\", encoding=\"utf-8-sig\") as f:\n",
    "    txt = f.read()\n",
    "\n",
    "VOCAB = sorted(list(set(txt)))\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "encode_dict = {c:i for i, c in enumerate(VOCAB)}\n",
    "decode_dict = {i:c for c, i in encode_dict.items()}\n",
    "\n",
    "def tok_encode(text):\n",
    "    return [encode_dict[c] for c in text]\n",
    "def tok_decode(tok_indices):\n",
    "    return \"\".join(decode_dict[e] for e in tok_indices)\n",
    "\n",
    "tok_decode(tok_encode(txt)) == txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "data = torch.LongTensor(tok_encode(txt)).to(DEVICE)\n",
    "split = int(len(data) * 0.9)\n",
    "train_data = data[:split]\n",
    "val_data = data[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Language Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from src import bigram\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_nn_smol = bigram.BigramLanguageModel(vocab_sz=VOCAB_SIZE).to(DEVICE)\n",
    "\n",
    "num_epochs = 16\n",
    "batch_size = 64\n",
    "dataset = bigram.BigramDataset(txt_tensor=train_data, device=DEVICE)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "optimizer = torch.optim.AdamW(model_nn_smol.parameters(), lr=1e-4)\n",
    "\n",
    "num_samples = []\n",
    "samples_counter = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for num_epoch in range(num_epochs):\n",
    "    for ii, (x, y) in enumerate(dataloader):\n",
    "        loss = model_nn_smol.train_batch(x, y, optimizer)\n",
    "        samples_counter += len(x)\n",
    "\n",
    "        if (ii+1) % 1000 == 0 or ii + 1 == len(dataloader):\n",
    "            with torch.no_grad():\n",
    "                train_loss = model_nn_smol.compute_loss(train_data[:-1], train_data[1:])\n",
    "                val_loss = model_nn_smol.compute_loss(val_data[:-1], val_data[1:])\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                num_samples.append(samples_counter)\n",
    "\n",
    "            print(\"\\r\" * 100 + f\"epoch {num_epoch}: {ii+1}/{len(dataloader)}\", end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import *\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict({\n",
    "    \"num_samples\": num_samples,\n",
    "    \"train_losses\": train_losses,\n",
    "    \"val_losses\": val_losses,\n",
    "}).pivot_longer(column_names=[\"train_losses\", \"val_losses\"], names_to=\"what\")\n",
    "\n",
    "ggplot(df) + geom_line(aes(x=\"num_samples\", y=\"value\", color=\"what\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn_smol.compute_loss(train_data[:-1], train_data[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lut = model_nn_smol.lut.weight.detach().to(\"cpu\").softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lektion\n",
    "\n",
    "- str.count() zählt keine überlappenden Muster!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram lut manuell\n",
    "\n",
    "lut_counting = torch.zeros((VOCAB_SIZE, VOCAB_SIZE))\n",
    "for c1, c2 in zip(train_data, train_data[1:]):\n",
    "    lut_counting[c1, c2] += 1\n",
    "# lut_counting = lut_counting.softmax(dim=-1)\n",
    "lut_counting = lut_counting / lut_counting.norm(dim=1, keepdim=True, p=1)\n",
    "lut_counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lut[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lut_counting[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "idx = np.random.randint(low=0, high=VOCAB_SIZE, size=(100,))\n",
    "out = \"\"\n",
    "for ii in range(len(idx)):\n",
    "    out += tok_decode([idx[ii]])\n",
    "\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_indices = model_nn_smol.generate(torch.tensor([0], device=DEVICE), 64)\n",
    "tok_decode(tok_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn_smol.compute_loss(val_data[:-1], val_data[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-np.log(1/VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Language Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smol Neural Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import neural\n",
    "model_nn_smol = neural.NeuralLanguageModel(\n",
    "    vocab_sz=VOCAB_SIZE,\n",
    "    embedding_size=84,\n",
    "    lin_size=64,\n",
    ")\n",
    "model_nn_smol.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "num_epochs = 16\n",
    "batch_size = 64\n",
    "dataset = neural.NeuralDataset(txt_tensor=train_data, device=DEVICE)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "optimizer = torch.optim.AdamW(model_nn_smol.parameters(), lr=1e-5)\n",
    "\n",
    "num_samples = []\n",
    "samples_counter = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_data_x = torch.stack([train_data[ii:ii+model_nn_smol.context_size] for ii in range(len(train_data)-3)])\n",
    "train_data_y = train_data[model_nn_smol.context_size:]\n",
    "val_data_x = torch.stack([val_data[ii:ii+model_nn_smol.context_size] for ii in range(len(val_data)-3)])\n",
    "val_data_y = val_data[model_nn_smol.context_size:]\n",
    "\n",
    "for num_epoch in range(num_epochs):\n",
    "    for ii, (x, y) in enumerate(dataloader):\n",
    "        loss = model_nn_smol.train_batch(x, y, optimizer)\n",
    "        samples_counter += len(x)\n",
    "\n",
    "        if (ii+1) % 1000 == 0 or ii + 1 == len(dataloader):\n",
    "            with torch.no_grad():\n",
    "                train_loss = model_nn_smol.compute_loss(train_data_x, train_data_y)\n",
    "                val_loss = model_nn_smol.compute_loss(val_data_x, val_data_y)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                num_samples.append(samples_counter)\n",
    "\n",
    "            print(\"\\r\" * 100 + f\"epoch {num_epoch}: {ii+1}/{len(dataloader)}\", end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import janitor\n",
    "from plotnine import *\n",
    "df = pd.DataFrame.from_dict({\n",
    "    \"num_samples\": num_samples,\n",
    "    \"train_losses\": train_losses,\n",
    "    \"val_losses\": val_losses,\n",
    "}).pivot_longer(column_names=[\"train_losses\", \"val_losses\"], names_to=\"what\")\n",
    "\n",
    "ggplot(df) + geom_line(aes(x=\"num_samples\", y=\"value\", color=\"what\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn_smol.compute_loss(train_data_x, train_data_y), model_nn_smol.compute_loss(val_data_x, val_data_y),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=torch.tensor(tok_encode(\"Hi \"), dtype=torch.long, device=DEVICE)\n",
    "tok = model_nn_smol.generate(prompt, 1024)\n",
    "print(tok_decode(tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swole Neural Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from src import neural\n",
    "model_nn_swole = neural.NeuralLanguageModel(\n",
    "    vocab_sz=VOCAB_SIZE,\n",
    "    embedding_size=384,\n",
    "    lin_size=512,\n",
    "    context_size=16,\n",
    "    nonlin = F.relu,\n",
    ")\n",
    "model_nn_swole.to(DEVICE)\n",
    "\n",
    "sum(p.numel() for p in model_nn_swole.parameters()), \\\n",
    "    model_nn_swole.compute_loss(train_data_x, train_data_y), model_nn_swole.compute_loss(val_data_x, val_data_y),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "num_epochs = 16\n",
    "batch_size = 64\n",
    "dataset = neural.NeuralDataset(txt_tensor=train_data, device=DEVICE, context_size=model_nn_swole.context_size)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "optimizer = torch.optim.AdamW(model_nn_swole.parameters(), lr=1e-7)\n",
    "\n",
    "num_samples = []\n",
    "samples_counter = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_data_x = torch.stack([train_data[ii:ii+model_nn_swole.context_size] for ii in range(len(train_data)-model_nn_swole.context_size)])\n",
    "train_data_y = train_data[model_nn_swole.context_size:]\n",
    "val_data_x = torch.stack([val_data[ii:ii+model_nn_swole.context_size] for ii in range(len(val_data)-model_nn_swole.context_size)])\n",
    "val_data_y = val_data[model_nn_swole.context_size:]\n",
    "\n",
    "for num_epoch in range(num_epochs):\n",
    "    for ii, (x, y) in enumerate(dataloader):\n",
    "        loss = model_nn_swole.train_batch(x, y, optimizer)\n",
    "        samples_counter += len(x)\n",
    "\n",
    "        if (ii+1) % 1000 == 0 or ii + 1 == len(dataloader):\n",
    "            with torch.no_grad():\n",
    "                train_loss = model_nn_swole.compute_loss(train_data_x, train_data_y)\n",
    "                val_loss = model_nn_swole.compute_loss(val_data_x, val_data_y)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                num_samples.append(samples_counter)\n",
    "\n",
    "            print(\"\\r\" * 100 + f\"epoch {num_epoch}: {ii+1}/{len(dataloader)}, {train_loss=:.2f}, {val_loss=:.2f}\", end=\"\", flush=True)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import janitor\n",
    "from plotnine import *\n",
    "df = pd.DataFrame.from_dict({\n",
    "    \"num_samples\": num_samples,\n",
    "    \"train_losses\": train_losses,\n",
    "    \"val_losses\": val_losses,\n",
    "}).pivot_longer(column_names=[\"train_losses\", \"val_losses\"], names_to=\"what\")\n",
    "\n",
    "ggplot(df) + geom_line(aes(x=\"num_samples\", y=\"value\", color=\"what\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn_swole.compute_loss(train_data_x, train_data_y), model_nn_swole.compute_loss(val_data_x, val_data_y),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=torch.tensor(tok_encode(\"Is this the real life?\"[:model_nn_swole.context_size]), dtype=torch.long, device=DEVICE)\n",
    "tok = model_nn_swole.generate(prompt, 1024)\n",
    "print(tok_decode(tok))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeTransformer = transformer.MakeTransformer(\n",
    "    transformerClass=transformer.Transformer,\n",
    "    textCorpus=txt,\n",
    "    numLayers=5,\n",
    "    embeddingSize=350,\n",
    "    headSize=7,\n",
    "    blockSize=64,\n",
    "    linScale=3,\n",
    "    dropout=0.2,\n",
    "    maxIters=5000,\n",
    "    learningRate=0.00005,\n",
    "    batchSize=64,\n",
    "    evalInterval=500,\n",
    "    evalIters=200,\n",
    ")\n",
    "sum([p.numel() for p in makeTransformer.model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeTransformer.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(makeTransformer.model.generate(\". \", 500))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
