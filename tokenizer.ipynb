{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(words, encode_dict): # words: [\"ahoi\", \"matey\", ...\"]\n",
    "    sorted_tokens = sorted(encode_dict.items(), key=lambda item: len(item[0]), reverse=True)\n",
    "    token_ids_array = []\n",
    "    for w in words:\n",
    "        token_ids = []\n",
    "        ptr = 0\n",
    "        while ptr < len(w):\n",
    "            for token, idx in sorted_tokens:\n",
    "                if w[ptr:].startswith(token):\n",
    "                    token_ids.append(idx)\n",
    "                    ptr += len(token)\n",
    "                    break\n",
    "\n",
    "        token_ids_array.append(token_ids)\n",
    "    return token_ids_array\n",
    "\n",
    "def bigram_counts(word_token_ids): # elements: [[1, 4, 22], [2, 44, 0], ...]\n",
    "    bigram_counts = {}\n",
    "\n",
    "    for tokens in word_token_ids:\n",
    "        for a, b in zip(tokens, tokens[1:]):\n",
    "            bigram_counts[(a, b)] = bigram_counts.get((a, b), 0) + 1\n",
    "\n",
    "    return bigram_counts\n",
    "\n",
    "# txt=\"\"\"\n",
    "# When Aunt Em came there to live she was a young, pretty wife. The sun\n",
    "# and wind had changed her, too. They had taken the sparkle from her eyes\n",
    "# and left them a sober gray; they had taken the red from her cheeks and\n",
    "# lips, and they were gray also. She was thin and gaunt, and never smiled\n",
    "# now. When Dorothy, who was an orphan, first came to her, Aunt Em had\n",
    "# \"\"\"\n",
    "with open(\"./wizard-of-oz.txt\", encoding=\"utf-8-sig\") as f:\n",
    "    txt = f.read()\n",
    "chars = sorted(list(set(txt)))\n",
    "encode_dict = {c:i for i,c in enumerate(chars)}\n",
    "decode_dict = {i:c for c,i in encode_dict.items()}\n",
    "\n",
    "words = sorted(list(set(txt.split(\" \"))))\n",
    "if \"\" in words:\n",
    "    words.remove(\"\")\n",
    "\n",
    "for _ in range(1000):\n",
    "    word_token_ids = tokenize_words(words, encode_dict)\n",
    "    if all([len(ids) == 1 for ids in word_token_ids]):\n",
    "        print(\"\\n\\nexhausted all words\")\n",
    "        break\n",
    "\n",
    "    counts = bigram_counts(word_token_ids)\n",
    "    most_common_bigram_idx = sorted(counts.items(), key=lambda item: item[1], reverse=True)[0][0]\n",
    "    most_common_bigram = \"\".join([decode_dict[idx] for idx in most_common_bigram_idx])\n",
    "\n",
    "    encode_dict[most_common_bigram] = len(encode_dict)\n",
    "    decode_dict = {i:c for c,i in encode_dict.items()}\n",
    "\n",
    "    print(f\"{most_common_bigram!r}, \", end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Is this the real life\"\n",
    "# 1, 4, 5, 2, 0, 1\n",
    "# -> (1, 4) = 69\n",
    "\n",
    "# \"Is this the real life\"\n",
    "# -> 69, 5, 2, 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_words([\"PArus.\\n\\n\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
