{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(words, encode_dict): # words: [\"ahoi\", \"matey\", ...\"]\n",
    "    sorted_tokens = sorted(encode_dict.items(), key=lambda item: len(item[0]), reverse=True)\n",
    "    token_ids_array = []\n",
    "    for w in words:\n",
    "        token_ids = []\n",
    "        ptr = 0\n",
    "        while ptr < len(w):\n",
    "            for token, idx in sorted_tokens:\n",
    "                if w[ptr:].startswith(token):\n",
    "                    token_ids.append(idx)\n",
    "                    ptr += len(token)\n",
    "                    break\n",
    "\n",
    "        token_ids_array.append(token_ids)\n",
    "    return token_ids_array\n",
    "\n",
    "def bigram_counts(word_token_ids): # elements: [[1, 4, 22], [2, 44, 0], ...]\n",
    "    bigram_counts = {}\n",
    "\n",
    "    for tokens in word_token_ids:\n",
    "        for a, b in zip(tokens, tokens[1:]):\n",
    "            bigram_counts[(a, b)] = bigram_counts.get((a, b), 0) + 1\n",
    "\n",
    "    return bigram_counts\n",
    "\n",
    "# txt=\"\"\"\n",
    "# When Aunt Em came there to live she was a young, pretty wife. The sun\n",
    "# and wind had changed her, too. They had taken the sparkle from her eyes\n",
    "# and left them a sober gray; they had taken the red from her cheeks and\n",
    "# lips, and they were gray also. She was thin and gaunt, and never smiled\n",
    "# now. When Dorothy, who was an orphan, first came to her, Aunt Em had\n",
    "# \"\"\"\n",
    "with open(\"./wizard-of-oz.txt\", encoding=\"utf-8-sig\") as f:\n",
    "    txt = f.read()\n",
    "chars = sorted(list(set(txt)))\n",
    "encode_dict = {c:i for i,c in enumerate(chars)}\n",
    "decode_dict = {i:c for c,i in encode_dict.items()}\n",
    "\n",
    "words = sorted(list(set(txt.split(\" \"))))\n",
    "if \"\" in words:\n",
    "    words.remove(\"\")\n",
    "\n",
    "for _ in range(1000):\n",
    "    word_token_ids = tokenize_words(words, encode_dict)\n",
    "    if all([len(ids) == 1 for ids in word_token_ids]):\n",
    "        print(\"\\n\\nexhausted all words\")\n",
    "        break\n",
    "\n",
    "    counts = bigram_counts(word_token_ids)\n",
    "    most_common_bigram_idx = sorted(counts.items(), key=lambda item: item[1], reverse=True)[0][0]\n",
    "    most_common_bigram = \"\".join([decode_dict[idx] for idx in most_common_bigram_idx])\n",
    "\n",
    "    encode_dict[most_common_bigram] = len(encode_dict)\n",
    "    decode_dict = {i:c for c,i in encode_dict.items()}\n",
    "\n",
    "    print(f\"{most_common_bigram!r}, \", end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt=\"\"\"\n",
    "# When Aunt Em came there to live she was a young, pretty wife. The sun\n",
    "# and wind had changed her, too. They had taken the sparkle from her eyes\n",
    "# and left them a sober gray; they had taken the red from her cheeks and\n",
    "# lips, and they were gray also. She was thin and gaunt, and never smiled\n",
    "# now. When Dorothy, who was an orphan, first came to her, Aunt Em had\n",
    "# \"\"\"\n",
    "with open(\"./wizard-of-oz.txt\", encoding=\"utf-8-sig\") as f:\n",
    "    txt = f.read()\n",
    "chars = sorted(list(set(txt)))\n",
    "encode_dict = {c:i for i,c in enumerate(chars)}\n",
    "decode_dict = {i:c for c,i in encode_dict.items()}\n",
    "\n",
    "words = sorted(list(set(txt.split(\" \"))))\n",
    "if \"\" in words:\n",
    "    words.remove(\"\")\n",
    "\n",
    "word_token_ids = tokenize_words(words, encode_dict)\n",
    "for _ in range(10000):\n",
    "    counts = bigram_counts(word_token_ids)\n",
    "    new_bigram_tuple = sorted(counts.items(), key=lambda item: item[1], reverse=True)[0][0] # (1, 4)\n",
    "    new_token_idx = len(encode_dict)\n",
    "\n",
    "    for word_ in word_token_ids:\n",
    "        ii = 0\n",
    "        while ii < len(word_) - 1:\n",
    "            if word_[ii] == new_bigram_tuple[0] and word_[ii+1] == new_bigram_tuple[1]:\n",
    "                word_[ii:ii+2] = [new_token_idx]\n",
    "                ii += 1\n",
    "            ii += 1\n",
    "\n",
    "    # print(new_bigram_tuple)\n",
    "    most_common_bigram = \"\".join([decode_dict[idx] for idx in new_bigram_tuple])\n",
    "    if encode_dict.get(most_common_bigram, None) is not None:\n",
    "        print(f\"{most_common_bigram=} existiert schon ({new_bigram_tuple=})\")\n",
    "    encode_dict[most_common_bigram] = new_token_idx\n",
    "    decode_dict = {i:c for c,i in encode_dict.items()}\n",
    "\n",
    "    if ii % 100 == 0:\n",
    "        print(\"\\r\"*100 + f\"{ii:}, {most_common_bigram!r}, \", end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_dict[43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(decode_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if encode_dict.get(\" \", None) is None:\n",
    "    encode_dict[\" \"] = len(encode_dict)\n",
    "    decode_dict[encode_dict[\" \"]] = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tok_encode(txt):\n",
    "    sorted_tokens = sorted(encode_dict.items(), key=lambda item: len(item[0]), reverse=True)\n",
    "    token_ids = []\n",
    "    ptr = 0\n",
    "    while ptr < len(txt):\n",
    "        for token, idx in sorted_tokens:\n",
    "            if txt[ptr:].startswith(token):\n",
    "                token_ids.append(idx)\n",
    "                ptr += len(token)\n",
    "                break\n",
    "\n",
    "    return token_ids\n",
    "\n",
    "def tok_decode(tok_ids):\n",
    "    return \"\".join([decode_dict[i] for i in tok_ids])\n",
    "\n",
    "tok_decode(tok_encode(\"Is this the real life?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
